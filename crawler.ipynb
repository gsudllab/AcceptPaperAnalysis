{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T06:15:48.265071Z",
     "start_time": "2019-09-07T06:15:48.261353Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T04:11:25.263578Z",
     "start_time": "2019-09-07T04:11:24.471963Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_html(url):#获得页面html代码\n",
    "    req = requests.get(url)\n",
    "    html = req.text\n",
    "    return html\n",
    "\n",
    "def crawl_list(url, pattern):\n",
    "    html = get_html(url)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    title = soup.find('main')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://neurips.cc/Conferences/2019/AcceptedPapersInitial\"\n",
    "html = get_html(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find target elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T06:10:41.873074Z",
     "start_time": "2019-09-07T06:10:41.860361Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppose I can always use three step to find target elements\n",
    "# 1. Find the first element contains paper\n",
    "# 2. Find the container contains all papers, For ICLR which uses different containers for oral/poster/reject, just find all oral papers, or all poster papers in one step.\n",
    "# 3. List those elements\n",
    "\n",
    "def find_target_elements(html, pattern):\n",
    "    eles = pattern.split(\"/\")\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    lists = soup.select(eles[0])\n",
    "    for l in lists:\n",
    "        if l.text == first:\n",
    "            break\n",
    "\n",
    "    if eles[1] == '..':\n",
    "        container = l\n",
    "        for i in range(len(eles[0].split(\">\"))):\n",
    "            container = container.parent\n",
    "    if '@' in eles[2]:\n",
    "        selector, index = eles[2].split(\"@\")\n",
    "    container_lists = container.select(selector)\n",
    "    if ':' not in index:\n",
    "        nodes = container_lists[int(index) - 1]\n",
    "    else:\n",
    "        start, end = index.split(\":\")\n",
    "        start = (int(start) - 1) if start else 0\n",
    "        end = (int(end) - 1) if end else len(container_lists)\n",
    "        nodes = container_lists[start:end]\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T06:10:57.824891Z",
     "start_time": "2019-09-07T06:10:57.517287Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "nodes = find_target_elements(html, \"p>b/../p@3:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser\n",
    "\n",
    "Usually the regular expression is not power enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T05:54:33.759409Z",
     "start_time": "2019-09-07T05:54:33.754930Z"
    }
   },
   "outputs": [],
   "source": [
    "def author_parser(text):\n",
    "    institute_list = []\n",
    "    left = 0\n",
    "    institute = \"\"\n",
    "    for c in text:\n",
    "        if c == '(':\n",
    "            left += 1\n",
    "            if left == 1:\n",
    "                institute = institute.strip()\n",
    "                institute_list.append(institute)\n",
    "                institute = \"\"\n",
    "        if c == ')':\n",
    "            left -= 1\n",
    "            continue\n",
    "        if left > 0 or c == '·':\n",
    "            continue\n",
    "        institute += c\n",
    "    return institute_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T06:11:16.152100Z",
     "start_time": "2019-09-07T06:11:16.141317Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def institute_parser(text):\n",
    "    institute_list = []\n",
    "    left = 0\n",
    "    institute = [\"\"]\n",
    "    for c in text:\n",
    "        if c == '(':\n",
    "            left += 1\n",
    "            if left == 1:\n",
    "                continue\n",
    "        elif c == ')':\n",
    "            left -= 1\n",
    "            if left > 0:\n",
    "                continue\n",
    "            if left == 0:\n",
    "                institute[-1] = institute[-1].strip()\n",
    "                institute_list.append(institute)\n",
    "                institute = [\"\"]\n",
    "        if left == 0 or left > 1:\n",
    "            continue\n",
    "        if c in ['&', '/']:\n",
    "            institute[-1] = institute[-1].strip()\n",
    "            institute.append('')\n",
    "            continue\n",
    "        institute[-1] += c\n",
    "    return institute_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T06:12:24.952708Z",
     "start_time": "2019-09-07T06:12:24.576649Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_item(element, rules):\n",
    "    paper_item = {}\n",
    "    extra_item = []\n",
    "    for key in rules:\n",
    "        if isinstance(rules[key], str):\n",
    "            paper_item[key] = element.select(rules[key])[0].text\n",
    "        elif isinstance(rules[key], list):\n",
    "            text = element.select(rules[key][0])[0].text\n",
    "\n",
    "            # Let's only consider author and/or institute\n",
    "            if key == 'authors':\n",
    "                if isinstance(rules[key][1]['author'], str):\n",
    "                    author_list = re.findall(rules[key][1]['author'], text)\n",
    "                else:\n",
    "                    author_list = rules[key][1]['author'](text)\n",
    "                if isinstance(rules[key][1]['institute'], str):\n",
    "                    institute_list = re.findall(rules[key][1]['institute'], text)\n",
    "                else:\n",
    "                    institute_list = rules[key][1]['institute'](text)\n",
    "                item = tuple(zip(author_list, institute_list))\n",
    "                assert len(author_list) == len(institute_list)\n",
    "            else:\n",
    "                print(\"not authors\")\n",
    "                raise NotImplementedError\n",
    "            paper_item[key] = item\n",
    "    return paper_item\n",
    "\n",
    "# (Facebook AI research (FAIR)) makes regular expression fail\n",
    "rules = {'title': 'p>b', 'authors': ['i', {'author': ' ?([^(·]+) \\([^(]+\\)', 'institute': '[^(·]+ \\(([^(]+)\\)'}]}\n",
    "rules = {'title': 'p>b', 'authors': ['i', {'author': author_parser, 'institute': institute_parser}]}\n",
    "\n",
    "paper_set = {}\n",
    "for element in nodes:\n",
    "    item = extract_item(element, rules)\n",
    "    title = item.pop('title')\n",
    "    paper_set[title] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T06:12:32.447672Z",
     "start_time": "2019-09-07T06:12:32.442094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1429"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T06:17:34.338918Z",
     "start_time": "2019-09-07T06:17:33.126311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1429\n"
     ]
    }
   ],
   "source": [
    "def main(url):\n",
    "\n",
    "    html = get_html(url)\n",
    "\n",
    "    nodes = find_target_elements(html, \"p>b/../p@3:\")\n",
    "    rules = {'title': 'p>b', 'authors': ['i', {'author': author_parser, 'institute': institute_parser}]}\n",
    "\n",
    "    paper_set = {}\n",
    "    for element in nodes:\n",
    "        item = extract_item(element, rules)\n",
    "        title = item.pop('title')\n",
    "        paper_set[title] = item\n",
    "    return paper_set\n",
    "\n",
    "url = \"https://neurips.cc/Conferences/2019/AcceptedPapersInitial\"\n",
    "paper_set = main(url)\n",
    "print(len(paper_set))\n",
    "\n",
    "with open('data/neurips2019.json', 'w') as fp:\n",
    "    json.dump(paper_set, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
